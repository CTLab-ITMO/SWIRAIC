{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# https://drive.google.com/file/d/1L5CEcaxIGiMYi-Zfugh_V1yqdeF_aUVM/view?usp=sharing\n!conda install -y gdown\n!gdown --id \"1L5CEcaxIGiMYi-Zfugh_V1yqdeF_aUVM\"\n!unzip -q Minerals.zip\n!rm Minerals.zip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os;\nimport torch;\nimport torch.fft;\nimport torchvision;\nimport numpy as np;\nfrom PIL import Image;\nfrom typing import Tuple;\nfrom random import randint;\nfrom torch import nn, optim;\nfrom torch.utils import data;\nimport torch.nn.functional as F;\nfrom torch.backends import cudnn;\nfrom torch.utils.data import Dataset;\nfrom torch.nn.parameter import Parameter;\nimport torchvision.transforms.functional as FT;","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:18:04.604438Z","iopub.execute_input":"2025-04-23T17:18:04.604747Z","iopub.status.idle":"2025-04-23T17:18:07.188895Z","shell.execute_reply.started":"2025-04-23T17:18:04.604722Z","shell.execute_reply":"2025-04-23T17:18:07.188315Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"cudnn.benchmark = True;\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\");\n\nbatch_size = 64;\nlearning_rate = 1e-3;\nepochs = 4000;","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:18:07.190197Z","iopub.execute_input":"2025-04-23T17:18:07.190942Z","iopub.status.idle":"2025-04-23T17:18:07.222064Z","shell.execute_reply.started":"2025-04-23T17:18:07.190919Z","shell.execute_reply":"2025-04-23T17:18:07.221503Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class DMlp(nn.Module):\n\n    def __init__(self, dim: int, growth_rate: float = 2.0) -> None:\n        super().__init__();\n        hidden_dim = int(dim * growth_rate);\n        self.conv_0 = nn.Sequential(\n            nn.Conv2d(dim, hidden_dim, 3, 1, 1, groups = dim),\n            nn.Conv2d(hidden_dim, hidden_dim, 1, 1, 0)\n        );\n        self.act = nn.GELU();\n        self.conv_1 = nn.Conv2d(hidden_dim, dim, 1, 1, 0);\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.conv_0(x);\n        x = self.act(x);\n        x = self.conv_1(x);\n        return x;\n\nclass PCFN(nn.Module):\n\n    def __init__(self, dim: int, growth_rate: float = 2.0, p_rate: float = 0.25) -> None:\n        super().__init__();\n        hidden_dim = int(dim * growth_rate);\n        p_dim = int(hidden_dim * p_rate);\n        self.conv_0 = nn.Conv2d(dim, hidden_dim, 1, 1, 0);\n        self.conv_1 = nn.Conv2d(p_dim, p_dim, 3, 1, 1);\n        self.act = nn.GELU();\n        self.conv_2 = nn.Conv2d(hidden_dim, dim, 1, 1, 0);\n        self.p_dim = p_dim;\n        self.hidden_dim = hidden_dim;\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if(self.training):\n            x = self.act(self.conv_0(x));\n            x1, x2 = torch.split(x, [self.p_dim, self.hidden_dim - self.p_dim], dim = 1);\n            x1 = self.act(self.conv_1(x1));\n            x = self.conv_2(torch.cat([x1, x2], dim = 1));\n        else:\n            x = self.act(self.conv_0(x));\n            x[:, :self.p_dim, :, :] = self.act(self.conv_1(x[:, :self.p_dim, :, :]));\n            x = self.conv_2(x);\n        return x;\n\nclass SMFA(nn.Module):\n\n    def __init__(self, dim: int = 36) -> None:\n        super().__init__();\n        self.linear_0 = nn.Conv2d(dim, dim * 2, 1, 1, 0);\n        self.linear_1 = nn.Conv2d(dim, dim, 1, 1, 0);\n        self.linear_2 = nn.Conv2d(dim, dim, 1, 1, 0);\n        self.lde = DMlp(dim, 2);\n        self.dw_conv = nn.Conv2d(dim, dim, 3, 1, 1, groups = dim);\n        self.gelu = nn.GELU();\n        self.down_scale = 8;\n        self.alpha = nn.Parameter(torch.ones((1, dim, 1, 1)));\n        self.beta = nn.Parameter(torch.zeros((1, dim, 1, 1)));\n\n    def forward(self, f: torch.Tensor) -> torch.Tensor:\n        _, _, h, w = f.shape;\n        y, x = self.linear_0(f).chunk(2, dim = 1);\n        x_s = self.dw_conv(F.adaptive_max_pool2d(x, (h // self.down_scale, w // self.down_scale)));\n        x_v = torch.var(x, dim = (-2, -1), keepdim = True);\n        x_l = x * F.interpolate(self.gelu(self.linear_1(x_s * self.alpha + x_v * self.beta)), size = (h, w), mode = 'nearest');\n        y_d = self.lde(y);\n        return self.linear_2(x_l + y_d);\n\nclass FMB(nn.Module):\n\n    def __init__(self, dim: int, ffn_scale: float = 2.0) -> None:\n        super().__init__();\n        self.smfa = SMFA(dim);\n        self.pcfn = PCFN(dim, ffn_scale);\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.smfa(F.normalize(x)) + x;\n        x = self.pcfn(F.normalize(x)) + x;\n        return x;\n \nclass SMFANet(nn.Module):\n\n    def __init__(self, dim: int = 48, n_blocks: int = 8, ffn_scale: float = 2.0, upscaling_factor: int = 4) -> None:\n        super().__init__();\n        self.scale = upscaling_factor;\n        self.to_feat = nn.Conv2d(3, dim, 3, 1, 1);\n        self.feats = nn.Sequential(*[FMB(dim, ffn_scale) for _ in range(n_blocks)]);\n        self.to_img = nn.Sequential(\n            nn.Conv2d(dim, 3 * upscaling_factor ** 2, 3, 1, 1),\n            nn.PixelShuffle(upscaling_factor)\n        );\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        res = F.interpolate(x, size = (x.size(2) * 4, x.size(3) * 4), mode = \"bilinear\");\n        x = self.to_feat(x);\n        x = self.feats(x) + x;\n        x = self.to_img(x);\n        return x + res;","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:18:07.222912Z","iopub.execute_input":"2025-04-23T17:18:07.223211Z","iopub.status.idle":"2025-04-23T17:18:07.353743Z","shell.execute_reply.started":"2025-04-23T17:18:07.223181Z","shell.execute_reply":"2025-04-23T17:18:07.353114Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def transform(input: torch.Tensor, target: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n    if(randint(0, 10) > 5):\n        input = FT.vflip(input);\n        target = FT.vflip(target);\n    if(randint(0, 10) > 5):\n        input = FT.hflip(input);\n        target = FT.hflip(target);\n    angle = randint(-30, 30);\n    input = FT.rotate(input, angle, interpolation = torchvision.transforms.InterpolationMode.BILINEAR);\n    target = FT.rotate(target, angle, interpolation = torchvision.transforms.InterpolationMode.BILINEAR);\n    return (input, target);\n\nclass MineralDataset(Dataset):\n\n    def __init__(self, root_dir: str, transform) -> None:\n        self.root_dir = root_dir;\n        self.transform = transform;\n        self.img_64 = [];\n        self.img_256 = [];\n        for idx in range(334):\n            img_64 = torch.cat([FT.to_tensor(Image.open(os.path.join(self.root_dir, \"64\", \"800\", f\"{(idx + 1):03}_800.jpg\")).convert('L')),\n                                FT.to_tensor(Image.open(os.path.join(self.root_dir, \"64\", \"1050\", f\"{(idx + 1):03}_1050.jpg\")).convert('L')),\n                                FT.to_tensor(Image.open(os.path.join(self.root_dir, \"64\", \"1550\", f\"{(idx + 1):03}_1550.jpg\")).convert('L'))], dim = 0);\n            img_256 = torch.cat([FT.to_tensor(Image.open(os.path.join(self.root_dir, \"256\", \"800\", f\"{(idx + 1):03}_800.jpg\")).convert('L')),\n                                 FT.to_tensor(Image.open(os.path.join(self.root_dir, \"256\", \"1050\", f\"{(idx + 1):03}_1050.jpg\")).convert('L')),\n                                 FT.to_tensor(Image.open(os.path.join(self.root_dir, \"256\", \"1550\", f\"{(idx + 1):03}_1550.jpg\")).convert('L'))], dim = 0);\n            self.img_64.append(img_64);\n            self.img_256.append(img_256);\n\n    def __len__(self) -> int:\n        return 334;\n\n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        return self.transform(self.img_64[idx], self.img_256[idx]);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:18:07.355134Z","iopub.execute_input":"2025-04-23T17:18:07.355426Z","iopub.status.idle":"2025-04-23T17:18:07.373957Z","shell.execute_reply.started":"2025-04-23T17:18:07.355401Z","shell.execute_reply":"2025-04-23T17:18:07.373323Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class FFTLoss(nn.Module):\n\n    def __init__(self, loss_weight: float = 1.0, reduction: str = 'mean') -> None:\n        super().__init__();\n        self.loss_weight = loss_weight;\n        self.criterion = torch.nn.L1Loss(reduction = reduction);\n\n    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        pred_fft = torch.fft.rfft2(pred);\n        target_fft = torch.fft.rfft2(target);\n        pred_fft = torch.stack([pred_fft.real, pred_fft.imag], dim = -1);\n        target_fft = torch.stack([target_fft.real, target_fft.imag], dim = -1);\n        return self.loss_weight * (self.criterion(pred_fft, target_fft));","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:18:07.374544Z","iopub.execute_input":"2025-04-23T17:18:07.374728Z","iopub.status.idle":"2025-04-23T17:18:07.393949Z","shell.execute_reply.started":"2025-04-23T17:18:07.374712Z","shell.execute_reply":"2025-04-23T17:18:07.393343Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def main():\n    dataset = MineralDataset('Minerals', transform);\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = True);\n    net = SMFANet().train().to(device);\n    opt_net = optim.Adam(net.parameters(), lr = learning_rate);\n    fft = FFTLoss(0.1);\n    avg_pix_loss = 0.0;\n    avg_fft_loss = 0.0;\n    it = 0;\n    for i in range(epochs):\n        for lr, hr in iter(dataloader):\n            lr = lr.to(device);\n            hr = hr.to(device);\n\n            sr = net(lr);\n            \n            pix_loss = F.l1_loss(sr, hr);\n            fft_loss = fft(sr, hr);\n            avg_pix_loss += pix_loss.item();\n            avg_fft_loss += fft_loss.item();\n            total_loss = pix_loss + fft_loss;\n            opt_net.zero_grad();\n            total_loss.backward();\n            opt_net.step();\n            it += 1;\n        if(((i + 1) % 100) == 0):\n            print(\"epoch: {:d}, pix: {:f}, fft: {:f}\".format(i + 1, avg_pix_loss / it, avg_fft_loss / it));\n            torch.save(net.state_dict(), 'smfanet_{:d}.pth'.format(i + 1));\n            avg_pix_loss = 0.0;\n            avg_fft_loss = 0.0;\n            it = 0;\n        if(((i + 1) % 400) == 0):\n            for param_group in opt_net.param_groups:\n                param_group['lr'] /= 2.0;\n\nmain();","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:18:07.394723Z","iopub.execute_input":"2025-04-23T17:18:07.394885Z"}},"outputs":[{"name":"stdout","text":"epoch: 100, pix: 0.067405, fft: 0.400208\nepoch: 200, pix: 0.058782, fft: 0.346130\nepoch: 300, pix: 0.055572, fft: 0.323497\nepoch: 400, pix: 0.054174, fft: 0.311843\nepoch: 500, pix: 0.052730, fft: 0.302955\nepoch: 600, pix: 0.052397, fft: 0.298569\nepoch: 700, pix: 0.051848, fft: 0.293731\nepoch: 800, pix: 0.051654, fft: 0.290456\nepoch: 900, pix: 0.051052, fft: 0.286622\nepoch: 1000, pix: 0.050863, fft: 0.285137\nepoch: 1100, pix: 0.050707, fft: 0.283876\nepoch: 1200, pix: 0.050620, fft: 0.282791\nepoch: 1300, pix: 0.050253, fft: 0.281285\nepoch: 1400, pix: 0.050178, fft: 0.280643\nepoch: 1500, pix: 0.050009, fft: 0.279866\nepoch: 1600, pix: 0.050010, fft: 0.279591\nepoch: 1700, pix: 0.049772, fft: 0.278819\nepoch: 1800, pix: 0.049777, fft: 0.278622\nepoch: 1900, pix: 0.049754, fft: 0.278467\nepoch: 2000, pix: 0.049688, fft: 0.278051\nepoch: 2100, pix: 0.049552, fft: 0.277659\nepoch: 2200, pix: 0.049547, fft: 0.277469\nepoch: 2300, pix: 0.049539, fft: 0.277497\nepoch: 2400, pix: 0.049524, fft: 0.277293\nepoch: 2500, pix: 0.049402, fft: 0.276821\nepoch: 2600, pix: 0.049379, fft: 0.276869\nepoch: 2700, pix: 0.049386, fft: 0.276887\nepoch: 2800, pix: 0.049354, fft: 0.276869\nepoch: 2900, pix: 0.049324, fft: 0.276630\nepoch: 3000, pix: 0.049308, fft: 0.276601\nepoch: 3100, pix: 0.049293, fft: 0.276496\nepoch: 3200, pix: 0.049277, fft: 0.276582\nepoch: 3300, pix: 0.049252, fft: 0.276512\nepoch: 3400, pix: 0.049284, fft: 0.276457\nepoch: 3500, pix: 0.049201, fft: 0.276109\nepoch: 3600, pix: 0.049231, fft: 0.276278\nepoch: 3700, pix: 0.049236, fft: 0.276459\nepoch: 3800, pix: 0.049261, fft: 0.276394\nepoch: 3900, pix: 0.049194, fft: 0.276148\nepoch: 4000, pix: 0.049204, fft: 0.276315\n","output_type":"stream"}],"execution_count":null}]}
